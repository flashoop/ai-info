{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Movie Review Sentiment Classification\n",
    "\n",
    "**Goal:** Train a model to predict whether a movie review is positive (1) or negative (0)\n",
    "\n",
    "**Approach:** Logistic Regression with TF-IDF vectorization\n",
    "\n",
    "This notebook follows a similar structure to the SMS spam classification project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Loading and Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Importing libraries...\")\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "with open('train.json', 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Load the test dataset\n",
    "with open('test.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"✓ Training samples loaded: {len(train_data)}\")\n",
    "print(f\"✓ Test samples loaded: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "train_df = pd.DataFrame(train_data)\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"\\n-------------------- HEAD --------------------\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\n-------------------- DESCRIBE --------------------\")\n",
    "print(train_df.describe())\n",
    "\n",
    "print(\"\\n-------------------- INFO --------------------\")\n",
    "print(train_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nDuplicate entries: {train_df.duplicated().sum()}\")\n",
    "\n",
    "# Remove duplicates if any\n",
    "if train_df.duplicated().sum() > 0:\n",
    "    train_df = train_df.drop_duplicates()\n",
    "    print(f\"✓ Removed {train_df.duplicated().sum()} duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "print(\"\\nLabel Distribution:\")\n",
    "print(train_df['label'].value_counts())\n",
    "\n",
    "# Visualize label distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "train_df['label'].value_counts().plot(kind='bar', color=['salmon', 'lightblue'])\n",
    "plt.title('Distribution of Positive vs Negative Reviews')\n",
    "plt.xlabel('Label (0=Negative, 1=Positive)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text lengths\n",
    "train_df['text_length'] = train_df['text'].apply(len)\n",
    "train_df['word_count'] = train_df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(\"\\nText Statistics:\")\n",
    "print(train_df[['text_length', 'word_count']].describe())\n",
    "\n",
    "# Average words per review by sentiment\n",
    "print(\"\\nAverage word count by label:\")\n",
    "print(train_df.groupby('label')['word_count'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Preprocessing the Dataset\n",
    "\n",
    "Similar to the SMS spam preprocessing, we will:\n",
    "1. Lowercase the text\n",
    "2. Remove HTML tags and unwanted characters\n",
    "3. Tokenize\n",
    "4. Remove stop words\n",
    "5. Apply stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing the Text\n",
    "\n",
    "Converting all text to lowercase ensures consistency and reduces dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PREPROCESSING PIPELINE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n=== BEFORE PREPROCESSING ===\")\n",
    "print(train_df['text'].head(2).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase the text\n",
    "train_df['text'] = train_df['text'].str.lower()\n",
    "test_df['text'] = test_df['text'].str.lower()\n",
    "\n",
    "print(\"\\n=== AFTER LOWERCASING ===\")\n",
    "print(train_df['text'].head(2).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing HTML Tags and Special Characters\n",
    "\n",
    "Movie reviews often contain HTML tags like `<br />`. We'll remove these along with unnecessary punctuation and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove HTML tags and special characters\"\"\"\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<br\\s*/?>', ' ', text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Keep only letters and basic punctuation\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(clean_text)\n",
    "test_df['text'] = test_df['text'].apply(clean_text)\n",
    "\n",
    "print(\"\\n=== AFTER REMOVING HTML & SPECIAL CHARACTERS ===\")\n",
    "print(train_df['text'].head(2).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization, Stop Word Removal, and Stemming\n",
    "\n",
    "These steps normalize the text by:\n",
    "- Breaking text into individual words (tokens)\n",
    "- Removing common words that don't add meaning (stop words)\n",
    "- Reducing words to their root form (stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "print(\"✓ NLTK resources downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stop words and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Complete preprocessing pipeline\"\"\"\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words and apply stemming\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    # Join back into string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"Applying full preprocessing pipeline...\")\n",
    "train_df['processed_text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['processed_text'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"\\n=== AFTER FULL PREPROCESSING ===\")\n",
    "print(\"Original:\", train_df['text'].iloc[0][:200])\n",
    "print(\"Processed:\", train_df['processed_text'].iloc[0][:200])\n",
    "print(\"\\n✓ Preprocessing completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Feature Extraction with TF-IDF\n",
    "\n",
    "We use TF-IDF (Term Frequency-Inverse Document Frequency) vectorization, which:\n",
    "- Captures both word frequency and importance\n",
    "- Includes unigrams and bigrams\n",
    "- Filters out very common and very rare terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE EXTRACTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_df['processed_text'],\n",
    "    train_df['label'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=train_df['label']\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Training set: {len(X_train)} samples\")\n",
    "print(f\"✓ Validation set: {len(X_val)} samples\")\n",
    "print(f\"✓ Test set: {len(test_df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=10000,  # Keep top 10k features\n",
    "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "    min_df=5,            # Ignore terms appearing in < 5 documents\n",
    "    max_df=0.8,          # Ignore terms appearing in > 80% of documents\n",
    ")\n",
    "\n",
    "print(\"\\nFitting TF-IDF vectorizer...\")\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = vectorizer.transform(X_val)\n",
    "\n",
    "print(f\"\\n✓ TF-IDF matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"✓ Number of features: {len(vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Model Training with Hyperparameter Tuning\n",
    "\n",
    "Similar to the SMS spam classification, we use GridSearchCV to find the best hyperparameters.\n",
    "We'll use Logistic Regression and tune the regularization parameter C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL TRAINING WITH HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(\n",
    "        max_features=10000,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=5,\n",
    "        max_df=0.8\n",
    "    )),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "print(\"✓ Pipeline created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__C': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0],  # Regularization strength\n",
    "    'classifier__penalty': ['l2']  # Regularization type\n",
    "}\n",
    "\n",
    "print(\"\\nParameter grid for tuning:\")\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search with 5-fold cross-validation\n",
    "print(\"\\nPerforming GridSearchCV (this may take a few minutes)...\")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BEST MODEL PARAMETERS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best F1 score (CV): {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 5: Model Evaluation\n",
    "\n",
    "Evaluate the model on validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on validation set\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "y_val_proba = best_model.predict_proba(X_val)\n",
    "\n",
    "# Calculate metrics\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred)\n",
    "val_recall = recall_score(y_val, y_val_pred)\n",
    "val_f1 = f1_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"\\nVALIDATION SET RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Accuracy:  {val_accuracy:.4f}\")\n",
    "print(f\"Precision: {val_precision:.4f}\")\n",
    "print(f\"Recall:    {val_recall:.4f}\")\n",
    "print(f\"F1-Score:  {val_f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for validation set\n",
    "cm_val = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Validation Set - Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_validation.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Confusion matrix saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test set\n",
    "y_test_pred = best_model.predict(test_df['processed_text'])\n",
    "y_test_proba = best_model.predict_proba(test_df['processed_text'])\n",
    "\n",
    "# Calculate metrics\n",
    "test_accuracy = accuracy_score(test_df['label'], y_test_pred)\n",
    "test_precision = precision_score(test_df['label'], y_test_pred)\n",
    "test_recall = recall_score(test_df['label'], y_test_pred)\n",
    "test_f1 = f1_score(test_df['label'], y_test_pred)\n",
    "\n",
    "print(\"\\nTEST SET RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Accuracy:  {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall:    {test_recall:.4f}\")\n",
    "print(f\"F1-Score:  {test_f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_df['label'], y_test_pred, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for test set\n",
    "cm_test = confusion_matrix(test_df['label'], y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Test Set - Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix_test.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Confusion matrix saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6: Feature Importance Analysis\n",
    "\n",
    "Analyze which words are most indicative of positive vs negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Get feature names and coefficients\n",
    "vectorizer_from_pipeline = best_model.named_steps['vectorizer']\n",
    "classifier_from_pipeline = best_model.named_steps['classifier']\n",
    "\n",
    "feature_names = vectorizer_from_pipeline.get_feature_names_out()\n",
    "coefficients = classifier_from_pipeline.coef_[0]\n",
    "\n",
    "# Top positive features\n",
    "top_positive_idx = np.argsort(coefficients)[-20:]\n",
    "top_positive = [(feature_names[i], coefficients[i]) for i in top_positive_idx]\n",
    "\n",
    "# Top negative features\n",
    "top_negative_idx = np.argsort(coefficients)[:20]\n",
    "top_negative = [(feature_names[i], coefficients[i]) for i in top_negative_idx]\n",
    "\n",
    "print(\"\\nTop 20 words indicating POSITIVE sentiment:\")\n",
    "for word, coef in reversed(top_positive):\n",
    "    print(f\"  {word:25s} {coef:8.4f}\")\n",
    "\n",
    "print(\"\\nTop 20 words indicating NEGATIVE sentiment:\")\n",
    "for word, coef in top_negative:\n",
    "    print(f\"  {word:25s} {coef:8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 7: Testing on New Examples\n",
    "\n",
    "Test the model with custom movie review examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TESTING ON CUSTOM EXAMPLES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Example movie reviews\n",
    "example_reviews = [\n",
    "    \"This movie was absolutely fantastic! I loved every minute of it. Best film I've seen this year!\",\n",
    "    \"Terrible movie. Complete waste of time and money. The acting was horrible and the plot made no sense.\",\n",
    "    \"It was okay, nothing special but not terrible either. Just an average movie.\",\n",
    "    \"A masterpiece! The direction, acting, and cinematography were all perfect. Highly recommended!\",\n",
    "    \"Boring and predictable. I fell asleep halfway through. Don't waste your time on this garbage.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and predict\n",
    "def predict_review(review_text):\n",
    "    \"\"\"Predict sentiment for a single review\"\"\"\n",
    "    # Apply same preprocessing\n",
    "    cleaned = clean_text(review_text.lower())\n",
    "    processed = preprocess_text(cleaned)\n",
    "    \n",
    "    # Predict\n",
    "    prediction = best_model.predict([processed])[0]\n",
    "    probabilities = best_model.predict_proba([processed])[0]\n",
    "    \n",
    "    return prediction, probabilities\n",
    "\n",
    "# Test each example\n",
    "for i, review in enumerate(example_reviews, 1):\n",
    "    pred, proba = predict_review(review)\n",
    "    sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "    confidence = proba[pred] * 100\n",
    "    \n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Prediction: {sentiment}\")\n",
    "    print(f\"Confidence: {confidence:.2f}%\")\n",
    "    print(f\"Probabilities: Negative={proba[0]:.2f}, Positive={proba[1]:.2f}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 8: Saving the Model\n",
    "\n",
    "Save the trained model using joblib for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SAVING MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Save the model\n",
    "model_filename = 'skills_assessment.joblib'\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "print(f\"\\n✓ Model saved to: {model_filename}\")\n",
    "\n",
    "# Get file size\n",
    "import os\n",
    "file_size = os.path.getsize(model_filename)\n",
    "print(f\"✓ File size: {file_size / 1024:.2f} KB ({file_size:,} bytes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "results = {\n",
    "    'model_name': 'IMDB Sentiment Classification',\n",
    "    'algorithm': 'Logistic Regression with TF-IDF',\n",
    "    'best_params': grid_search.best_params_,\n",
    "    'validation': {\n",
    "        'accuracy': float(val_accuracy),\n",
    "        'precision': float(val_precision),\n",
    "        'recall': float(val_recall),\n",
    "        'f1_score': float(val_f1)\n",
    "    },\n",
    "    'test': {\n",
    "        'accuracy': float(test_accuracy),\n",
    "        'precision': float(test_precision),\n",
    "        'recall': float(test_recall),\n",
    "        'f1_score': float(test_f1)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('skills_assessment_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ Results saved to: skills_assessment_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 9: Loading and Testing the Saved Model\n",
    "\n",
    "Demonstrate how to load and use the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LOADING SAVED MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = joblib.load(model_filename)\n",
    "\n",
    "print(f\"\\n✓ Model loaded from: {model_filename}\")\n",
    "print(f\"✓ Model type: {type(loaded_model).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the loaded model\n",
    "test_review = \"This film is amazing and wonderful! A true masterpiece of cinema.\"\n",
    "\n",
    "# Preprocess\n",
    "cleaned = clean_text(test_review.lower())\n",
    "processed = preprocess_text(cleaned)\n",
    "\n",
    "# Predict with loaded model\n",
    "prediction = loaded_model.predict([processed])[0]\n",
    "probabilities = loaded_model.predict_proba([processed])[0]\n",
    "\n",
    "print(\"\\nTest with loaded model:\")\n",
    "print(f\"Review: {test_review}\")\n",
    "print(f\"Prediction: {'Positive' if prediction == 1 else 'Negative'}\")\n",
    "print(f\"Confidence: {probabilities[prediction] * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✓ TRAINING PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implements an IMDB sentiment classification model following the same structure as the SMS spam classification project:\n",
    "\n",
    "1. **Data Loading & Exploration**: Loaded 25,000 train + 25,000 test reviews\n",
    "2. **Preprocessing**: Lowercasing, HTML removal, tokenization, stop word removal, stemming\n",
    "3. **Feature Extraction**: TF-IDF with unigrams and bigrams (10,000 features)\n",
    "4. **Model Training**: Logistic Regression with GridSearchCV hyperparameter tuning\n",
    "5. **Evaluation**: Comprehensive metrics on validation and test sets\n",
    "6. **Feature Analysis**: Identified most important positive/negative indicators\n",
    "7. **Model Saving**: Saved as `skills_assessment.joblib` for deployment\n",
    "\n",
    "The model achieves strong performance and is ready for use in sentiment classification tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
