{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dedf557f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful\n",
      "Extraction successful\n",
      "Extracted files: ['readme', 'SMSSpamCollection']\n",
      "-------------------- HEAD --------------------\n",
      "  label                                            message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "-------------------- DESCRIBE --------------------\n",
      "       label                 message\n",
      "count   5572                    5572\n",
      "unique     2                    5169\n",
      "top      ham  Sorry, I'll call later\n",
      "freq    4825                      30\n",
      "-------------------- INFO --------------------\n",
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype\n",
      "---  ------   --------------  -----\n",
      " 0   label    5572 non-null   str  \n",
      " 1   message  5572 non-null   str  \n",
      "dtypes: str(2)\n",
      "memory usage: 87.2 KB\n",
      "None\n",
      "Missing values:\n",
      " label      0\n",
      "message    0\n",
      "dtype: int64\n",
      "Duplicate entries: 403\n"
     ]
    }
   ],
   "source": [
    "%run spam_class.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2ca45fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BEFORE ANY PREPROCESSING ===\n",
      "  label                                            message\n",
      "0   ham  go until jurong point crazy available only in ...\n",
      "1   ham                            ok lar joking wif u oni\n",
      "2  spam  free entry in  a wkly comp to win fa cup final...\n",
      "3   ham        u dun say so early hor u c already then say\n",
      "4   ham  nah i dont think he goes to usf he lives aroun...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/leo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/leo/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/leo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the necessary NLTK data files\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "print(\"=== BEFORE ANY PREPROCESSING ===\") \n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76065107",
   "metadata": {},
   "source": [
    "low casing the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca963fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER LOWERCASING ===\n",
      "0    go until jurong point, crazy.. available only ...\n",
      "1                        ok lar... joking wif u oni...\n",
      "2    free entry in 2 a wkly comp to win fa cup fina...\n",
      "3    u dun say so early hor... u c already then say...\n",
      "4    nah i don't think he goes to usf, he lives aro...\n",
      "Name: message, dtype: str\n"
     ]
    }
   ],
   "source": [
    "# Convert all message text to lowercase\n",
    "df[\"message\"] = df[\"message\"].str.lower()\n",
    "print(\"\\n=== AFTER LOWERCASING ===\")\n",
    "print(df[\"message\"].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd7a508",
   "metadata": {},
   "source": [
    "Removing Punctuation and Numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5661a247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AFTER REMOVING PUNCTUATION & NUMBERS (except $ and !) ===\n",
      "0    go until jurong point crazy available only in ...\n",
      "1                              ok lar joking wif u oni\n",
      "2    free entry in  a wkly comp to win fa cup final...\n",
      "3          u dun say so early hor u c already then say\n",
      "4    nah i dont think he goes to usf he lives aroun...\n",
      "Name: message, dtype: str\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Remove non-essential punctuation and numbers, keep useful symbols like $ and !\n",
    "df[\"message\"] = df[\"message\"].apply(lambda x: re.sub(r\"[^a-z\\s$!]\", \"\", x))\n",
    "print(\"\\n=== AFTER REMOVING PUNCTUATION & NUMBERS (except $ and !) ===\")\n",
    "print(df[\"message\"].head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc68476",
   "metadata": {},
   "source": [
    "Using CountVectorizer for the Bag-of-Words Approach\n",
    "CountVectorizer from the scikit-learn library efficiently implements the bag-of-words approach. It converts a collection of documents into a matrix of term counts, where each row represents a message and each column corresponds to a term (unigram or bigram). Before transformation, CountVectorizer applies tokenization, builds a vocabulary, and then maps each document to a numeric vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9752d5d",
   "metadata": {},
   "source": [
    "Key parameters for refining the feature set:\n",
    "\n",
    "min_df=1: A term must appear in at least one document to be included. While this threshold is set to 1 here, higher values can be used in practice to exclude rare terms.\n",
    "max_df=0.9: Terms that appear in more than 90% of the documents are excluded, removing overly common words that provide limited differentiation.\n",
    "ngram_range=(1, 2): The feature matrix captures individual words and common word pairs by including unigrams and bigrams, potentially improving the model’s ability to detect spam patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acf92cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer with bigrams, min_df, and max_df to focus on relevant terms\n",
    "vectorizer = CountVectorizer(min_df=1, max_df=0.9, ngram_range=(1, 2))\n",
    "\n",
    "# Fit and transform the message column\n",
    "X = vectorizer.fit_transform(df[\"message\"])\n",
    "\n",
    "# Labels (target variable)\n",
    "y = df[\"label\"].apply(lambda x: 1 if x == \"spam\" else 0)  # Converting labels to 1 and 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a746fafd",
   "metadata": {},
   "source": [
    "How CountVectorizer Works\n",
    "CountVectorizer operates in three main stages:\n",
    "\n",
    "Tokenization: Splits the text into tokens based on the specified ngram_range. For ngram_range=(1, 2), it extracts both unigrams (like \"message\") and bigrams (like \"free prize\").\n",
    "Building the Vocabulary: Uses min_df and max_df to decide which terms to include. Terms that are too rare or common are filtered out, leaving a vocabulary that balances informative and distinctive terms.\n",
    "Vectorization: Transforms each document into a vector of term counts. Each vector entry corresponds to a term from the vocabulary, and its value represents how many times that term appears in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc4a008",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "595486e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Build the pipeline by combining vectorization and classification\n",
    "pipeline = Pipeline([\n",
    "    (\"vectorizer\", vectorizer),\n",
    "    (\"classifier\", MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5c41dd",
   "metadata": {},
   "source": [
    "To achieve this, we use GridSearchCV. This method systematically searches through specified hyperparameter values to identify the configuration that produces the best performance. In the case of MultinomialNB, we focus on the alpha parameter, a smoothing factor that adjusts how the model handles unseen words and prevents probabilities from being zero. We can balance bias and variance by tuning alpha, ultimately improving the model’s robustness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af0895e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model parameters: {'classifier__alpha': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    \"classifier__alpha\": [0.01, 0.1, 0.15, 0.2, 0.25, 0.5, 0.75, 1.0]\n",
    "}\n",
    "\n",
    "# Perform the grid search with 5-fold cross-validation and the F1-score as metric\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1\"\n",
    ")\n",
    "\n",
    "# Fit the grid search on the full dataset\n",
    "grid_search.fit(df[\"message\"], y)\n",
    "\n",
    "# Extract the best model identified by the grid search\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best model parameters:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63ca9295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example SMS messages for evaluation\n",
    "new_messages = [\n",
    "    \"Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\",\n",
    "    \"Hey, are we still meeting up for lunch today?\",\n",
    "    \"Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\",\n",
    "    \"Reminder: Your appointment is scheduled for tomorrow at 10am.\",\n",
    "    \"FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8569ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 依赖变量已就绪：stop_words 和 stemmer 已定义。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/leo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/leo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# 1. 下载 NLTK 资源（仅需运行一次）\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 2. 定义函数中引用的变量\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "print(\"✅ 依赖变量已就绪：stop_words 和 stemmer 已定义。\")\n",
    "\n",
    "\n",
    "# Preprocess function that mirrors the training-time preprocessing\n",
    "def preprocess_message(message):\n",
    "    message = message.lower()\n",
    "    message = re.sub(r\"[^a-z\\s$!]\", \"\", message)\n",
    "    tokens = word_tokenize(message)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4dfea08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and vectorize messages\n",
    "processed_messages = [preprocess_message(msg) for msg in new_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d65045",
   "metadata": {},
   "source": [
    "Vectorizing the Processed Messages\n",
    "The model expects numerical input features. To achieve this, we apply the same vectorization method used during training. The CountVectorizer saved within the pipeline (best_model.named_steps[\"vectorizer\"]) transforms the preprocessed text into a numerical feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "83e115ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform preprocessed messages into feature vectors\n",
    "X_new = best_model.named_steps[\"vectorizer\"].transform(processed_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9476b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the trained classifier\n",
    "predictions = best_model.named_steps[\"classifier\"].predict(X_new)\n",
    "prediction_probabilities = best_model.named_steps[\"classifier\"].predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2b4c1d",
   "metadata": {},
   "source": [
    "Displaying Predictions and Probabilities\n",
    "The next step is to present the evaluation results. For each message, we display:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3bdf97bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Congratulations! You've won a $1000 Walmart gift card. Go to http://bit.ly/1234 to claim now.\n",
      "Prediction: Spam\n",
      "Spam Probability: 0.99\n",
      "Not-Spam Probability: 0.01\n",
      "--------------------------------------------------\n",
      "Message: Hey, are we still meeting up for lunch today?\n",
      "Prediction: Not-Spam\n",
      "Spam Probability: 0.00\n",
      "Not-Spam Probability: 1.00\n",
      "--------------------------------------------------\n",
      "Message: Urgent! Your account has been compromised. Verify your details here: www.fakebank.com/verify\n",
      "Prediction: Spam\n",
      "Spam Probability: 0.96\n",
      "Not-Spam Probability: 0.04\n",
      "--------------------------------------------------\n",
      "Message: Reminder: Your appointment is scheduled for tomorrow at 10am.\n",
      "Prediction: Not-Spam\n",
      "Spam Probability: 0.01\n",
      "Not-Spam Probability: 0.99\n",
      "--------------------------------------------------\n",
      "Message: FREE entry in a weekly competition to win an iPad. Just text WIN to 80085 now!\n",
      "Prediction: Spam\n",
      "Spam Probability: 1.00\n",
      "Not-Spam Probability: 0.00\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display predictions and probabilities for each evaluated message\n",
    "for i, msg in enumerate(new_messages):\n",
    "    prediction = \"Spam\" if predictions[i] == 1 else \"Not-Spam\"\n",
    "    spam_probability = prediction_probabilities[i][1]  # Probability of being spam\n",
    "    ham_probability = prediction_probabilities[i][0]   # Probability of being not spam\n",
    "    \n",
    "    print(f\"Message: {msg}\")\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    print(f\"Spam Probability: {spam_probability:.2f}\")\n",
    "    print(f\"Not-Spam Probability: {ham_probability:.2f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8626fb1",
   "metadata": {},
   "source": [
    "Using joblib for Saving Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad25c350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to spam_detection_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model to a file for future use\n",
    "model_filename = 'spam_detection_model.joblib'\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "print(f\"Model saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63376e15",
   "metadata": {},
   "source": [
    "In this example, best_model likely refers to a finalized and tested pipeline or classifier. The file spam_detection_model.joblib will contain all the necessary information to predict new data. To reuse the model later, load it back into the environment. Remember that new data must be preprocessed the same way as the training data before making predictions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ccd54cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model = joblib.load(model_filename)\n",
    "\n",
    "# Preprocess new messages before prediction\n",
    "new_data_processed = [preprocess_message(msg) for msg in new_messages]\n",
    "\n",
    "# Make predictions on the preprocessed data\n",
    "predictions = loaded_model.predict(new_data_processed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
